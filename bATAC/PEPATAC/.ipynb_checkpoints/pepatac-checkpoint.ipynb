{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b4287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials  import Credentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c2e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[44]:\n",
    "# ID and range of a seq processing sheet\n",
    "                       # 1S_7ql04XgKSu3VTRRiOyrCmzP8GhOEhAzxdByhpgj1A\n",
    "SAMPLE_SPREADSHEET_ID = '1S_7ql04XgKSu3VTRRiOyrCmzP8GhOEhAzxdByhpgj1A'\n",
    "#SAMPLE_RANGE_NAME = 'A1:M10000'\n",
    "now = datetime.now()\n",
    "\n",
    "# In[47]:\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We need functions too:\n",
    "# 1. Read google sheet.\n",
    "# 2. Create pepatac datasheet from input rows \n",
    "# 3. Extract project name from google sheet given input rows \n",
    "# 4. Extract Input path from google sheet given input rows \n",
    "# 5. Parse datasheet to create R1, R2 paths from input path\n",
    "# 6. Edit yaml with relevant info\n",
    "\n",
    "### Note: See if we can add extra if organism then conditions to yaml\n",
    "\n",
    "### Extras \n",
    "# Format bash script to launch yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00026f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Edit yaml to incluse \n",
    "# 1. project name \n",
    "# 2. sample_table_path\n",
    "# 3. output_path\n",
    "# 4. R1 path\n",
    "# 5. R2 path\n",
    "# 6. organism\n",
    "# 7. genome index\n",
    "# 8. organism chrom sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c926cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_atac_sheet():\n",
    "    # ATAC sheet ID\n",
    "   # SAMPLE_SPREADSHEET_ID = '1S_7ql04XgKSu3VTRRiOyrCmzP8GhOEhAzxdByhpgj1A'\n",
    "   # SAMPLE_RANGE_NAME = 'A1:M10000'\n",
    "   # now = datetime.now()\n",
    "   # SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']\n",
    "    \n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('/projects/ps-epigen/users/rlan/ATACSeq/bot/token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('/projects/ps-epigen/users/rlan/ATACSeq/bot/token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                '/projects/ps-epigen/10x_output/Sequencing-Processing-Bot/python/credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=5734)\n",
    "        # Save the credentials for the next run\n",
    "        with open('/projects/ps-epigen/users/rlan/ATACSeq/bot/token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    # Call the Sheets API\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "                                range= 'PEPATAC!A1:M10000').execute()\n",
    "    values = result.get('values', [])\n",
    "\n",
    "    if not values:\n",
    "        print('No data found.')\n",
    "    else:\n",
    "        DF = pd.DataFrame(values[1:], columns=values[0])\n",
    "        \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e996c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = read_atac_sheet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a339479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2554b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Use sample name to grep input dir to aquire the true seq name for data sheet #########\n",
    "\n",
    "def create_pep_sheet(pandas_gs, start_row, stop_row):\n",
    "    # rows = 20:24\n",
    "    project = pandas_gs[start_row:stop_row]\n",
    "    \n",
    "    # 1. Create project dirs using the Base path + Data productions lead + project names\n",
    "    base_path = \"/projects/ps-epigen/users/rlan/ATACSeq/PEPATAC/FFS/\"\n",
    "    dlead = DF[start_row:start_row + 1].squeeze()[0]\n",
    "    pname = DF[start_row:start_row + 1].squeeze()[1]\n",
    "    proj_path = base_path + dlead + \"/\" + pname + \"/\"\n",
    "    os.makedirs(proj_path, exist_ok = True)\n",
    "    \n",
    "    d = {'sample_name': project['SEQ_ID'] , 'protocol': 'ATAC' , 'organism': project['Organism'], \n",
    "         'read1': 'R1' , 'read2': 'R2' , 'read_type': 'paired'}\n",
    "    pep_sheet = pd.DataFrame(d)\n",
    "    \n",
    "    #Use sample name to grep input dir to aquire the true seq name for data sheet\n",
    "    ipath = DF[start_row:start_row + 1].squeeze()[6]\n",
    " #   print(ipath)\n",
    "    #\"/projects/ps-epigen/users/rlan/ATACSeq/demultiplex/221005_NB501692_0223_AH23H5AFX5/out/AH23H5AFX5/\"\n",
    "    ifiles = os.listdir(ipath)\n",
    "    fbnames = list()\n",
    "\n",
    "    for s in pep_sheet['sample_name']:\n",
    "        fname = list(filter(lambda x: re.search(s + r\".*R1\", x), ifiles))\n",
    "        fbase_name = re.search(r'^.*_R', fname[0]).group(0)[:-2]\n",
    "        fbnames.append(fbase_name)\n",
    "\n",
    "    pep_sheet['sample_name'] = fbnames\n",
    "    pep_sheet.to_csv(path_or_buf= f\"{proj_path}datasheet.csv\", header=True, index = False)\n",
    "    \n",
    "    return pep_sheet\n",
    "\n",
    "# if sample not present in input dir \n",
    "# throw error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4f1dc9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/ps-epigen/users/rlan/ATACSeq/demultiplex/230105_NB501692_0239_AH5VCLAFX5/out/AH5VCLAFX5/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>protocol</th>\n",
       "      <th>organism</th>\n",
       "      <th>read1</th>\n",
       "      <th>read2</th>\n",
       "      <th>read_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>QY_1861_S11</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>QY_1862_S12</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>QY_1863_S13</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>QY_1864_S14</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>QY_1865_S15</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>QY_1866_S16</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>QY_1867_S17</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>QY_1868_S18</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>QY_1869_S19</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>QY_1870_S20</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>QY_1871_S21</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>QY_1872_S22</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>QY_1873_S23</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>QY_1874_S24</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>QY_1875_S25</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>QY_1876_S26</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>QY_1877_S27</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>QY_1878_S28</td>\n",
       "      <td>ATAC</td>\n",
       "      <td>human</td>\n",
       "      <td>R1</td>\n",
       "      <td>R2</td>\n",
       "      <td>paired</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_name protocol organism read1 read2 read_type\n",
       "168  QY_1861_S11     ATAC    human    R1    R2    paired\n",
       "169  QY_1862_S12     ATAC    human    R1    R2    paired\n",
       "170  QY_1863_S13     ATAC    human    R1    R2    paired\n",
       "171  QY_1864_S14     ATAC    human    R1    R2    paired\n",
       "172  QY_1865_S15     ATAC    human    R1    R2    paired\n",
       "173  QY_1866_S16     ATAC    human    R1    R2    paired\n",
       "174  QY_1867_S17     ATAC    human    R1    R2    paired\n",
       "175  QY_1868_S18     ATAC    human    R1    R2    paired\n",
       "176  QY_1869_S19     ATAC    human    R1    R2    paired\n",
       "177  QY_1870_S20     ATAC    human    R1    R2    paired\n",
       "178  QY_1871_S21     ATAC    human    R1    R2    paired\n",
       "179  QY_1872_S22     ATAC    human    R1    R2    paired\n",
       "180  QY_1873_S23     ATAC    human    R1    R2    paired\n",
       "181  QY_1874_S24     ATAC    human    R1    R2    paired\n",
       "182  QY_1875_S25     ATAC    human    R1    R2    paired\n",
       "183  QY_1876_S26     ATAC    human    R1    R2    paired\n",
       "184  QY_1877_S27     ATAC    human    R1    R2    paired\n",
       "185  QY_1878_S28     ATAC    human    R1    R2    paired"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pep_sheet = create_pep_sheet(DF, start_row=30, stop_row=42)\n",
    "#pep_sheet = create_pep_sheet(DF, start_row=65, stop_row=80)\n",
    "pep_sheet = create_pep_sheet(DF, start_row=168, stop_row=186)\n",
    "pep_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a44096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pepatac_shell(pep_DF, start_row):\n",
    "    # extract input dir\n",
    "    i_path = pep_DF[start_row:start_row + 1].squeeze()[6]\n",
    "    \n",
    "    base_path = \"/projects/ps-epigen/users/rlan/ATACSeq/PEPATAC/FFS/\"\n",
    "    dlead = DF[start_row:start_row + 1].squeeze()[0]\n",
    "    pname = DF[start_row:start_row + 1].squeeze()[1]\n",
    "    proj_path = base_path + dlead + \"/\" + pname + \"/\"\n",
    "    \n",
    "    # Path to yaml = project_path + project.yaml\n",
    "    ypath = proj_path + \"project.yaml\"\n",
    "    \n",
    "    # Replace strings in file with extracted idir & pdir\n",
    "    with open(\"/projects/ps-epigen/users/rlan/ATACSeq/bot/pepatac.temp.sh\", 'rt') as fh:\n",
    "        data = fh.read()\n",
    "        #replace all occurrences of the required string\n",
    "        data = data.replace('yaml_path', ypath)\n",
    "        \n",
    "       \n",
    "    with open(f\"{proj_path}pepatac.sh\", \"w\") as fh:\n",
    "        fh.write(data)\n",
    "        \n",
    "  #  print(proj_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60fb5f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#write_pepatac_shell(DF, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a89ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pepatac_yaml(pep_DF, start_row):\n",
    "    \n",
    "    base_path = \"/projects/ps-epigen/users/rlan/ATACSeq/PEPATAC/FFS/\"\n",
    "    dlead = DF[start_row:start_row + 1].squeeze()[0]\n",
    "    pname = DF[start_row:start_row + 1].squeeze()[1]\n",
    "    proj_path = base_path + dlead + \"/\" + pname + \"/\"\n",
    "    stable = proj_path + \"datasheet.csv\"\n",
    "    opath = proj_path + \"out\"\n",
    "    ipath = DF[start_row:start_row + 1].squeeze()[6]\n",
    "    \n",
    "    \n",
    "    # Replace strings in file with extracted idir & pdir\n",
    "    with open(\"/projects/ps-epigen/users/rlan/ATACSeq/bot/project.temp.yaml\", 'rt') as fh:\n",
    "        data = fh.read()\n",
    "        data = data.replace('Temp_Proj_Name', pname)\n",
    "        data = data.replace('sample_table_path', stable)\n",
    "        data = data.replace('output_path', opath)\n",
    "        data = data.replace('input_path', ipath)\n",
    "        \n",
    "    with open(f\"{proj_path}project.yaml\", \"w\") as fh:\n",
    "        fh.write(data)\n",
    "        \n",
    "        print(proj_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1d57b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_pepatac_yaml(DF, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1bb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For yaml we need \n",
    "# 1. Project Name\n",
    "# 2. sample table path\n",
    "# 3. output path\n",
    "# 4 R1_Path & R2_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions to create. \n",
    "# 1. Format project.yaml\n",
    "# 2. format pepatac.sh\n",
    "# 3. Run pepatac\n",
    "# 4. Find pepatac projects \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a50e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pep_projects(PDF):\n",
    "    ## Step 2. Identify rows of the sheet that are not completed\n",
    "    no_rows = PDF[PDF['Completed'] =='No']\n",
    "    \n",
    "    # 3. Among the not completed rows, identify start and stop rows for each project present\n",
    "    # 4. Create 2d list of each start, stop\n",
    "    projects = np.unique(no_rows['Project Name'])\n",
    "    proj_start_stops = list()\n",
    "\n",
    "    for p in projects:\n",
    "        tpdf = no_rows[no_rows['Project Name'] == p]\n",
    "        proj_index = tpdf.index\n",
    "        # + 1 because python shenanigans \n",
    "        start_stop = [proj_index[0], proj_index[-1] + 1]\n",
    "        proj_start_stops.append(start_stop)\n",
    "    \n",
    "    return(proj_start_stops, projects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087627cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[339, 343]], array(['Julia_QY_2134-2137_2'], dtype=object))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indys, projects = find_pep_projects(DF)\n",
    "indys, projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dbf508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Edit #####\n",
    "def run_pepatac(pep_DF, start_row):\n",
    "    # Format project path\n",
    "    base_path = \"/projects/ps-epigen/users/rlan/ATACSeq/PEPATAC/FFS/\"\n",
    "    dlead = DF[start_row:start_row + 1].squeeze()[0]\n",
    "    pname = DF[start_row:start_row + 1].squeeze()[1]\n",
    "    proj_path = base_path + dlead + \"/\" + pname + \"/\"\n",
    "    \n",
    "    # Format qsub\n",
    "    qsub_cmd = ['qsub', f\"{proj_path}pepatac.sh\"]\n",
    "   # print(qsub_cmd)\n",
    "    subprocess.run(qsub_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e183136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_pepatac(pep_DF=DF, start_row=186)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6c50785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/ps-epigen/users/rlan/ATACSeq/PEPATAC/FFS/Qian/Julia_QY_2134-2137_2/\n",
      "33104093.tscc-mgr7.local\n"
     ]
    }
   ],
   "source": [
    "for i in indys:\n",
    "    create_pep_sheet(DF, start_row=i[0], stop_row=i[1])\n",
    "    \n",
    "    write_pepatac_shell(DF, start_row=i[0])\n",
    "    \n",
    "    write_pepatac_yaml(DF, i[0])\n",
    "    \n",
    "    run_pepatac(DF, i[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ce014",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## NOTE #\n",
    "# In final version make os.makedirs(proj_path, exist_ok = True) exist_ok = FALSE to avoid previous output \n",
    "# from getting overwritten by sheet editor or pipeline errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c330373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
